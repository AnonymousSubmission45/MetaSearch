ðŸ“‚ Automated_Consensus_Analysis/
â”‚â”€â”€ ðŸ“œ README.md                      # Project overview, setup instructions, usage  
â”‚â”€â”€ ðŸ“œ requirements.txt                # Python dependencies  
â”‚â”€â”€ ðŸ“œ .gitignore                      # Ignore unnecessary files (e.g., datasets, logs, cache)  
â”‚â”€â”€ ðŸ“œ LICENSE                         # License information  
â”‚â”€â”€ ðŸ“œ repository_structure.txt         # Auto-generated repository structure  
â”‚â”€â”€ ðŸ“œ workflow_drawio.png              # Visual workflow diagram  
â”‚  
â”œâ”€â”€ ðŸ“‚ data/                            # Dataset storage and processing scripts  
â”‚   â”œâ”€â”€ ðŸ“œ aggregate.py                 # Aggregate extracted data  
â”‚   â”œâ”€â”€ ðŸ“œ convert_json_to_csv.py       # Convert dataset formats  
â”‚   â”œâ”€â”€ ðŸ“œ merge_final.py               # Merge finalized datasets  
â”‚   â”œâ”€â”€ ðŸ“œ merge_search_critique.py     # Merge search results with critiques  
â”‚   â”œâ”€â”€ ðŸ“œ prepare_new_subset.py        # Create a new subset of PeerSum  
â”‚   â”œâ”€â”€ ðŸ“œ prepare_sample_subset.py     # Generate a sample dataset  
â”‚   â”œâ”€â”€ ðŸ“‚ processed/                   # Processed dataset storage  
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ consensus_resolution/    # Consensus-related outputs  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ combined_final.csv  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ disagreement_resolution.csv  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ disagreement_resolution_final.csv  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ meta_reviews.csv  
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“œ meta_reviews_final.csv  
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ critique_points/         # Extracted critique points  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ critique_points_llm.json  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ critique_points_llm_2.json  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ critique_points_nlp.json  
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“œ critique_points_nlp_2.json  
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ disagreement_detection/  # Disagreement detection outputs  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ disagreement_aggregated.csv  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ disagreement_scores_llm.json  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ disagreement_scores_nlp.json  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ metrics.csv  
â”‚   â”‚   â””â”€â”€ ðŸ“‚ search/                  # Search-retrieved evidence  
â”‚   â”‚       â”œâ”€â”€ ðŸ“œ logs.json  
â”‚   â”‚       â”œâ”€â”€ ðŸ“œ papers_with_search_final.csv  
â”‚   â”‚       â”œâ”€â”€ ðŸ“œ search_final_2.json  
â”‚   â”‚       â”œâ”€â”€ ðŸ“œ search_intermediate.json  
â”‚   â”‚       â””â”€â”€ ðŸ“œ search_intermediate_2.json  
â”‚   â”œâ”€â”€ ðŸ“‚ raw/                         # Original dataset files  
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ data-00000-of-00001.arrow  
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ dataset_info.json  
â”‚   â”‚   â””â”€â”€ ðŸ“œ state.json  
â”‚   â”œâ”€â”€ ðŸ“‚ split/                       # Train/Test/Validation splits  
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ sample_subset_train.json  
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ test/                     # Test set  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ data-00000-of-00001.arrow  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ dataset_info.json  
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“œ state.json  
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ train/                    # Training set  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ data-00000-of-00001.arrow  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ dataset_info.json  
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“œ state.json  
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ val/                      # Validation set  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ data-00000-of-00001.arrow  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ dataset_info.json  
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“œ state.json  
â”‚   â””â”€â”€ ðŸ“œ split_dataset.py              # Split data into subsets  
â”‚  
â”œâ”€â”€ ðŸ“‚ docs/                            # Research papers and references  
â”‚   â”œâ”€â”€ ðŸ“œ Benchmark on Peer Review Toxic Detection.pdf  
â”‚   â”œâ”€â”€ ðŸ“œ Large Language Models Penetration in Scholarly Writing and Peer Review.pdf  
â”‚   â”œâ”€â”€ ðŸ“œ Paper Quality Assessment Individual Wisdom Metrics.pdf  
â”‚   â”œâ”€â”€ ðŸ“œ ReviewEval An Evaluation Framework for AI-Generated Reviews.pdf  
â”‚  
â”œâ”€â”€ ðŸ“‚ notebooks/                       # Jupyter notebooks for analysis  
â”‚   â”œâ”€â”€ ðŸ“œ 01_Exploratory_Analysis.ipynb  
â”‚   â”œâ”€â”€ ðŸ“œ 02_EDA_CritiqueLLM.ipynb  
â”‚   â”œâ”€â”€ ðŸ“œ 03_Disagreement_Scores_LLM.ipynb  
â”‚   â”œâ”€â”€ ðŸ“œ 04_Search_Result_Analysis.ipynb  
â”‚   â”œâ”€â”€ ðŸ“œ 05_Retrieved_Evidences_Analysis.ipynb  
â”‚   â”œâ”€â”€ ðŸ“œ 06_Disagreement_Resolution.ipynb  
â”‚   â”œâ”€â”€ ðŸ“œ 07_Meta_Review_Comparison.ipynb  
â”‚  
â”œâ”€â”€ ðŸ“‚ src/                             # Core codebase  
â”‚   â”œâ”€â”€ ðŸ“‚ consensus_resolution/        # LLM-based consensus synthesis  
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ disagreement_resolution_deepseek.py  
â”‚   â”‚   â””â”€â”€ ðŸ“œ meta_review_generation.py  
â”‚   â”œâ”€â”€ ðŸ“‚ critique_points_extraction/  # Extract critique points  
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ extract_critique_llm.py  
â”‚   â”‚   â””â”€â”€ ðŸ“œ extract_critique_nlp.py  
â”‚   â”œâ”€â”€ ðŸ“‚ disagreement_detection/      # Disagreement detection  
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ compare_reviews_llm.py  
â”‚   â”‚   â””â”€â”€ ðŸ“œ compare_reviews_nlp.py  
â”‚   â”œâ”€â”€ ðŸ“‚ search_retrieval/            # Search-Augmented Verification  
â”‚   â”‚   â””â”€â”€ ðŸ“œ search_agent.ipynb  
