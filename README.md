# Consensus Analysis in Academic Peer Review Using Search-Augmented Large Language Models

This repository contains the implementation and data processing pipeline for the paper "Consensus Analysis in Academic Peer Review Using Search-Augmented Large Language Models".

## Repository Structure

```
ðŸ“‚ Automated_Consensus_Analysis/
â”‚â”€â”€ ðŸ“œ README.md                      # Project overview, setup instructions, usage  
â”‚â”€â”€ ðŸ“œ requirements.txt                # Python dependencies  
â”‚â”€â”€ ðŸ“œ .gitignore                      # Ignore unnecessary files (e.g., datasets, logs, cache)  
â”‚â”€â”€ ðŸ“œ LICENSE                         # License information  
â”‚â”€â”€ ðŸ“œ repository_structure.txt         # Auto-generated repository structure  
â”‚  
â”œâ”€â”€ ðŸ“‚ data/                            # Dataset storage and processing scripts  
â”‚   â”œâ”€â”€ ðŸ“œ aggregate.py                 # Aggregate extracted data  
â”‚   â”œâ”€â”€ ðŸ“œ convert_json_to_csv.py       # Convert dataset formats  
â”‚   â”œâ”€â”€ ðŸ“œ merge_final.py               # Merge finalized datasets  
â”‚   â”œâ”€â”€ ðŸ“œ merge_search_critique.py     # Merge search results with critiques  
â”‚   â”œâ”€â”€ ðŸ“œ prepare_new_subset.py        # Create a new subset of PeerSum  
â”‚   â”œâ”€â”€ ðŸ“œ prepare_sample_subset.py     # Generate a sample dataset  
â”‚   â”œâ”€â”€ ðŸ“‚ processed/                   # Processed dataset storage  
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ consensus_resolution/    # Consensus-related outputs  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ combined_final.csv  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ disagreement_resolution.csv  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ disagreement_resolution_final.csv  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ meta_reviews.csv  
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“œ meta_reviews_final.csv  
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ critique_points/         # Extracted critique points  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ critique_points_llm.json  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ critique_points_llm_2.json  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ critique_points_nlp.json  
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“œ critique_points_nlp_2.json  
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ disagreement_detection/  # Disagreement detection outputs  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ disagreement_aggregated.csv  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ disagreement_scores_llm.json  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ disagreement_scores_nlp.json  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ metrics.csv  
â”‚   â”‚   â””â”€â”€ ðŸ“‚ search/                  # Search-retrieved evidence  
â”‚   â”‚       â”œâ”€â”€ ðŸ“œ logs.json  
â”‚   â”‚       â”œâ”€â”€ ðŸ“œ papers_with_search_final.csv  
â”‚   â”‚       â”œâ”€â”€ ðŸ“œ search_final_2.json  
â”‚   â”‚       â”œâ”€â”€ ðŸ“œ search_intermediate.json  
â”‚   â”‚       â””â”€â”€ ðŸ“œ search_intermediate_2.json  
â”‚   â”œâ”€â”€ ðŸ“‚ raw/                         # Original dataset files  
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ data-00000-of-00001.arrow  
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ dataset_info.json  
â”‚   â”‚   â””â”€â”€ ðŸ“œ state.json  
â”‚   â”œâ”€â”€ ðŸ“‚ split/                       # Train/Test/Validation splits  
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ sample_subset_train.json  
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ test/                     # Test set  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ data-00000-of-00001.arrow  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ dataset_info.json  
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“œ state.json  
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ train/                    # Training set  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ data-00000-of-00001.arrow  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ dataset_info.json  
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“œ state.json  
â”‚   â”‚   â”œâ”€â”€ ðŸ“‚ val/                      # Validation set  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ data-00000-of-00001.arrow  
â”‚   â”‚   â”‚   â”œâ”€â”€ ðŸ“œ dataset_info.json  
â”‚   â”‚   â”‚   â””â”€â”€ ðŸ“œ state.json  
â”‚   â””â”€â”€ ðŸ“œ split_dataset.py              # Split data into subsets  
â”‚  
â”œâ”€â”€ ðŸ“‚ docs/                            # Research papers and references  
â”‚   â”œâ”€â”€ ðŸ“œ Benchmark on Peer Review Toxic Detection.pdf  
â”‚   â”œâ”€â”€ ðŸ“œ Large Language Models Penetration in Scholarly Writing and Peer Review.pdf  
â”‚   â”œâ”€â”€ ðŸ“œ Paper Quality Assessment Individual Wisdom Metrics.pdf  
â”‚   â”œâ”€â”€ ðŸ“œ ReviewEval An Evaluation Framework for AI-Generated Reviews.pdf  
â”‚  
â”œâ”€â”€ ðŸ“‚ notebooks/                       # Jupyter notebooks for analysis  
â”‚   â”œâ”€â”€ ðŸ“œ 01_Exploratory_Analysis.ipynb  
â”‚   â”œâ”€â”€ ðŸ“œ 02_EDA_CritiqueLLM.ipynb  
â”‚   â”œâ”€â”€ ðŸ“œ 03_Disagreement_Scores_LLM.ipynb  
â”‚   â”œâ”€â”€ ðŸ“œ 04_Search_Result_Analysis.ipynb  
â”‚   â”œâ”€â”€ ðŸ“œ 05_Retrieved_Evidences_Analysis.ipynb  
â”‚   â”œâ”€â”€ ðŸ“œ 06_Disagreement_Resolution.ipynb  
â”‚   â”œâ”€â”€ ðŸ“œ 07_Meta_Review_Comparison.ipynb  
â”‚  
â”œâ”€â”€ ðŸ“‚ src/                             # Core codebase  
â”‚   â”œâ”€â”€ ðŸ“‚ consensus_resolution/        # LLM-based consensus synthesis  
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ disagreement_resolution_deepseek.py  
â”‚   â”‚   â””â”€â”€ ðŸ“œ meta_review_generation.py  
â”‚   â”œâ”€â”€ ðŸ“‚ critique_points_extraction/  # Extract critique points  
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ extract_critique_llm.py  
â”‚   â”‚   â””â”€â”€ ðŸ“œ extract_critique_nlp.py  
â”‚   â”œâ”€â”€ ðŸ“‚ disagreement_detection/      # Disagreement detection  
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ compare_reviews_llm.py  
â”‚   â”‚   â””â”€â”€ ðŸ“œ compare_reviews_nlp.py  
â”‚   â”œâ”€â”€ ðŸ“‚ search_retrieval/            # Search-Augmented Verification  
â”‚   â”‚   â””â”€â”€ ðŸ“œ search_agent.ipynb  
â”‚  
â””â”€â”€ ðŸ“œ workflow_drawio.png              # Visual workflow diagram  
```

## Data Processing Pipeline

The data processing workflow includes several Python scripts in the `data/` directory:

- `prepare_sample_subset.py`: Creates initial sample datasets
- `prepare_new_subset.py`: Generates additional data subsets
- `convert_json_to_csv.py`: Converts JSON data to CSV format
- `merge_search_critique.py`: Merges search results with critique data
- `merge_final.py`: Performs final data merging
- `aggregate.py`: Aggregates results
- `split_dataset.py`: Splits data into training and testing sets

## Methodology

Our approach follows a systematic workflow:

1. Data Collection and Preprocessing
2. Search-Augmented Review Analysis
3. Consensus Detection
4. Result Validation

For detailed workflow visualization, see [workflow_drawio.png](workflow_drawio.png).

## Setup

1. Create a Python virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Configure environment variables:
```bash
cp .env.example .env
# Edit .env with your configuration
```

## Documentation

For more detailed information, refer to the following documents in the `docs/` directory:
- Main Paper: "Consensus Analysis in Academic Peer Review Using Search-Augmented Large Language Models"
- Implementation Details: "Benchmark on Peer Review Toxic Detection.pdf"
- Research Context: "Large Language Models Penetration in Scholarly Writing and Peer Review.pdf"

## MLflow Tracking

Experiment tracking and model artifacts are stored in the `mlruns/` directory. Use MLflow UI to view experiments:

```bash
mlflow ui
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
